{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESas_eMON1Lu",
        "outputId": "febd0983-49bf-4290-a437-b8193e75d635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i3fUYkGN7uk",
        "outputId": "2b98a3d5-307e-4949-f1b7-6fade08859ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from transformer_decoder_model import TransformerDecoderModel, Config  # Import your model here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_6haVUMOFNR",
        "outputId": "ad76c5f3-b6e9-4b64-9650-71a67b33fb7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with 29.29M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = np.load(path)\n",
        "        self.inputs = torch.tensor(data['input_ids'], dtype=torch.long)\n",
        "        self.targets = torch.tensor(data['target_ids'], dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "kihCEq11OK0g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 3e-4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "MhRej2UdOOGf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "dataset = ChatDataset('chat_data_sequences.npz')\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "JdPoaMc4OQq2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model\n",
        "config = Config()\n",
        "model = TransformerDecoderModel(config).to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "lVaJodwzOTNg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        logits, loss = model(x, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{step}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"üéØ Epoch [{epoch+1}/{EPOCHS}] complete ‚Äî Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8-vB6eyOVpv",
        "outputId": "bdfbfb23-9f42-4998-8e23-28062687035e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step [0/10575], Loss: 9.3553\n",
            "Epoch [1/3], Step [100/10575], Loss: 0.5418\n",
            "Epoch [1/3], Step [200/10575], Loss: 0.1958\n",
            "Epoch [1/3], Step [300/10575], Loss: 0.1771\n",
            "Epoch [1/3], Step [400/10575], Loss: 0.1687\n",
            "Epoch [1/3], Step [500/10575], Loss: 0.1581\n",
            "Epoch [1/3], Step [600/10575], Loss: 0.1674\n",
            "Epoch [1/3], Step [700/10575], Loss: 0.1573\n",
            "Epoch [1/3], Step [800/10575], Loss: 0.1714\n",
            "Epoch [1/3], Step [900/10575], Loss: 0.1632\n",
            "Epoch [1/3], Step [1000/10575], Loss: 0.1649\n",
            "Epoch [1/3], Step [1100/10575], Loss: 0.1644\n",
            "Epoch [1/3], Step [1200/10575], Loss: 0.1601\n",
            "Epoch [1/3], Step [1300/10575], Loss: 0.1608\n",
            "Epoch [1/3], Step [1400/10575], Loss: 0.1640\n",
            "Epoch [1/3], Step [1500/10575], Loss: 0.1655\n",
            "Epoch [1/3], Step [1600/10575], Loss: 0.1611\n",
            "Epoch [1/3], Step [1700/10575], Loss: 0.1619\n",
            "Epoch [1/3], Step [1800/10575], Loss: 0.1629\n",
            "Epoch [1/3], Step [1900/10575], Loss: 0.1618\n",
            "Epoch [1/3], Step [2000/10575], Loss: 0.1614\n",
            "Epoch [1/3], Step [2100/10575], Loss: 0.1678\n",
            "Epoch [1/3], Step [2200/10575], Loss: 0.1497\n",
            "Epoch [1/3], Step [2300/10575], Loss: 0.1541\n",
            "Epoch [1/3], Step [2400/10575], Loss: 0.1520\n",
            "Epoch [1/3], Step [2500/10575], Loss: 0.1630\n",
            "Epoch [1/3], Step [2600/10575], Loss: 0.1562\n",
            "Epoch [1/3], Step [2700/10575], Loss: 0.1605\n",
            "Epoch [1/3], Step [2800/10575], Loss: 0.1538\n",
            "Epoch [1/3], Step [2900/10575], Loss: 0.1602\n",
            "Epoch [1/3], Step [3000/10575], Loss: 0.1600\n",
            "Epoch [1/3], Step [3100/10575], Loss: 0.1642\n",
            "Epoch [1/3], Step [3200/10575], Loss: 0.1686\n",
            "Epoch [1/3], Step [3300/10575], Loss: 0.1663\n",
            "Epoch [1/3], Step [3400/10575], Loss: 0.1671\n",
            "Epoch [1/3], Step [3500/10575], Loss: 0.1628\n",
            "Epoch [1/3], Step [3600/10575], Loss: 0.1520\n",
            "Epoch [1/3], Step [3700/10575], Loss: 0.1681\n",
            "Epoch [1/3], Step [3800/10575], Loss: 0.1571\n",
            "Epoch [1/3], Step [3900/10575], Loss: 0.1578\n",
            "Epoch [1/3], Step [4000/10575], Loss: 0.1611\n",
            "Epoch [1/3], Step [4100/10575], Loss: 0.1567\n",
            "Epoch [1/3], Step [4200/10575], Loss: 0.1641\n",
            "Epoch [1/3], Step [4300/10575], Loss: 0.1622\n",
            "Epoch [1/3], Step [4400/10575], Loss: 0.1529\n",
            "Epoch [1/3], Step [4500/10575], Loss: 0.1587\n",
            "Epoch [1/3], Step [4600/10575], Loss: 0.1560\n",
            "Epoch [1/3], Step [4700/10575], Loss: 0.1605\n",
            "Epoch [1/3], Step [4800/10575], Loss: 0.1628\n",
            "Epoch [1/3], Step [4900/10575], Loss: 0.1622\n",
            "Epoch [1/3], Step [5000/10575], Loss: 0.1605\n",
            "Epoch [1/3], Step [5100/10575], Loss: 0.1658\n",
            "Epoch [1/3], Step [5200/10575], Loss: 0.1541\n",
            "Epoch [1/3], Step [5300/10575], Loss: 0.1624\n",
            "Epoch [1/3], Step [5400/10575], Loss: 0.1599\n",
            "Epoch [1/3], Step [5500/10575], Loss: 0.1584\n",
            "Epoch [1/3], Step [5600/10575], Loss: 0.1602\n",
            "Epoch [1/3], Step [5700/10575], Loss: 0.1552\n",
            "Epoch [1/3], Step [5800/10575], Loss: 0.1657\n",
            "Epoch [1/3], Step [5900/10575], Loss: 0.1600\n",
            "Epoch [1/3], Step [6000/10575], Loss: 0.1525\n",
            "Epoch [1/3], Step [6100/10575], Loss: 0.1591\n",
            "Epoch [1/3], Step [6200/10575], Loss: 0.1622\n",
            "Epoch [1/3], Step [6300/10575], Loss: 0.1510\n",
            "Epoch [1/3], Step [6400/10575], Loss: 0.1572\n",
            "Epoch [1/3], Step [6500/10575], Loss: 0.1523\n",
            "Epoch [1/3], Step [6600/10575], Loss: 0.1591\n",
            "Epoch [1/3], Step [6700/10575], Loss: 0.1558\n",
            "Epoch [1/3], Step [6800/10575], Loss: 0.1539\n",
            "Epoch [1/3], Step [6900/10575], Loss: 0.1571\n",
            "Epoch [1/3], Step [7000/10575], Loss: 0.1627\n",
            "Epoch [1/3], Step [7100/10575], Loss: 0.1576\n",
            "Epoch [1/3], Step [7200/10575], Loss: 0.1589\n",
            "Epoch [1/3], Step [7300/10575], Loss: 0.1569\n",
            "Epoch [1/3], Step [7400/10575], Loss: 0.1631\n",
            "Epoch [1/3], Step [7500/10575], Loss: 0.1580\n",
            "Epoch [1/3], Step [7600/10575], Loss: 0.1587\n",
            "Epoch [1/3], Step [7700/10575], Loss: 0.1546\n",
            "Epoch [1/3], Step [7800/10575], Loss: 0.1592\n",
            "Epoch [1/3], Step [7900/10575], Loss: 0.1548\n",
            "Epoch [1/3], Step [8000/10575], Loss: 0.1604\n",
            "Epoch [1/3], Step [8100/10575], Loss: 0.1609\n",
            "Epoch [1/3], Step [8200/10575], Loss: 0.1608\n",
            "Epoch [1/3], Step [8300/10575], Loss: 0.1572\n",
            "Epoch [1/3], Step [8400/10575], Loss: 0.1602\n",
            "Epoch [1/3], Step [8500/10575], Loss: 0.1673\n",
            "Epoch [1/3], Step [8600/10575], Loss: 0.1602\n",
            "Epoch [1/3], Step [8700/10575], Loss: 0.1634\n",
            "Epoch [1/3], Step [8800/10575], Loss: 0.1583\n",
            "Epoch [1/3], Step [8900/10575], Loss: 0.1539\n",
            "Epoch [1/3], Step [9000/10575], Loss: 0.1609\n",
            "Epoch [1/3], Step [9100/10575], Loss: 0.1480\n",
            "Epoch [1/3], Step [9200/10575], Loss: 0.1582\n",
            "Epoch [1/3], Step [9300/10575], Loss: 0.1530\n",
            "Epoch [1/3], Step [9400/10575], Loss: 0.1557\n",
            "Epoch [1/3], Step [9500/10575], Loss: 0.1572\n",
            "Epoch [1/3], Step [9600/10575], Loss: 0.1568\n",
            "Epoch [1/3], Step [9700/10575], Loss: 0.1612\n",
            "Epoch [1/3], Step [9800/10575], Loss: 0.1583\n",
            "Epoch [1/3], Step [9900/10575], Loss: 0.1569\n",
            "Epoch [1/3], Step [10000/10575], Loss: 0.1522\n",
            "Epoch [1/3], Step [10100/10575], Loss: 0.1603\n",
            "Epoch [1/3], Step [10200/10575], Loss: 0.1517\n",
            "Epoch [1/3], Step [10300/10575], Loss: 0.1578\n",
            "Epoch [1/3], Step [10400/10575], Loss: 0.1500\n",
            "Epoch [1/3], Step [10500/10575], Loss: 0.1568\n",
            "üéØ Epoch [1/3] complete ‚Äî Average Loss: 0.1765\n",
            "Epoch [2/3], Step [0/10575], Loss: 0.1566\n",
            "Epoch [2/3], Step [100/10575], Loss: 0.1488\n",
            "Epoch [2/3], Step [200/10575], Loss: 0.1572\n",
            "Epoch [2/3], Step [300/10575], Loss: 0.1601\n",
            "Epoch [2/3], Step [400/10575], Loss: 0.1576\n",
            "Epoch [2/3], Step [500/10575], Loss: 0.1520\n",
            "Epoch [2/3], Step [600/10575], Loss: 0.1609\n",
            "Epoch [2/3], Step [700/10575], Loss: 0.1559\n",
            "Epoch [2/3], Step [800/10575], Loss: 0.1497\n",
            "Epoch [2/3], Step [900/10575], Loss: 0.1557\n",
            "Epoch [2/3], Step [1000/10575], Loss: 0.1553\n",
            "Epoch [2/3], Step [1100/10575], Loss: 0.1531\n",
            "Epoch [2/3], Step [1200/10575], Loss: 0.1545\n",
            "Epoch [2/3], Step [1300/10575], Loss: 0.1582\n",
            "Epoch [2/3], Step [1400/10575], Loss: 0.1572\n",
            "Epoch [2/3], Step [1500/10575], Loss: 0.1596\n",
            "Epoch [2/3], Step [1600/10575], Loss: 0.1585\n",
            "Epoch [2/3], Step [1700/10575], Loss: 0.1491\n",
            "Epoch [2/3], Step [1800/10575], Loss: 0.1514\n",
            "Epoch [2/3], Step [1900/10575], Loss: 0.1580\n",
            "Epoch [2/3], Step [2000/10575], Loss: 0.1434\n",
            "Epoch [2/3], Step [2100/10575], Loss: 0.1496\n",
            "Epoch [2/3], Step [2200/10575], Loss: 0.1496\n",
            "Epoch [2/3], Step [2300/10575], Loss: 0.1537\n",
            "Epoch [2/3], Step [2400/10575], Loss: 0.1510\n",
            "Epoch [2/3], Step [2500/10575], Loss: 0.1541\n",
            "Epoch [2/3], Step [2600/10575], Loss: 0.1510\n",
            "Epoch [2/3], Step [2700/10575], Loss: 0.1479\n",
            "Epoch [2/3], Step [2800/10575], Loss: 0.1543\n",
            "Epoch [2/3], Step [2900/10575], Loss: 0.1536\n",
            "Epoch [2/3], Step [3000/10575], Loss: 0.1499\n",
            "Epoch [2/3], Step [3100/10575], Loss: 0.1458\n",
            "Epoch [2/3], Step [3200/10575], Loss: 0.1451\n",
            "Epoch [2/3], Step [3300/10575], Loss: 0.1563\n",
            "Epoch [2/3], Step [3400/10575], Loss: 0.1419\n",
            "Epoch [2/3], Step [3500/10575], Loss: 0.1498\n",
            "Epoch [2/3], Step [3600/10575], Loss: 0.1499\n",
            "Epoch [2/3], Step [3700/10575], Loss: 0.1440\n",
            "Epoch [2/3], Step [3800/10575], Loss: 0.1458\n",
            "Epoch [2/3], Step [3900/10575], Loss: 0.1441\n",
            "Epoch [2/3], Step [4000/10575], Loss: 0.1479\n",
            "Epoch [2/3], Step [4100/10575], Loss: 0.1518\n",
            "Epoch [2/3], Step [4200/10575], Loss: 0.1458\n",
            "Epoch [2/3], Step [4300/10575], Loss: 0.1515\n",
            "Epoch [2/3], Step [4400/10575], Loss: 0.1447\n",
            "Epoch [2/3], Step [4500/10575], Loss: 0.1436\n",
            "Epoch [2/3], Step [4600/10575], Loss: 0.1448\n",
            "Epoch [2/3], Step [4700/10575], Loss: 0.1432\n",
            "Epoch [2/3], Step [4800/10575], Loss: 0.1420\n",
            "Epoch [2/3], Step [4900/10575], Loss: 0.1402\n",
            "Epoch [2/3], Step [5000/10575], Loss: 0.1400\n",
            "Epoch [2/3], Step [5100/10575], Loss: 0.1426\n",
            "Epoch [2/3], Step [5200/10575], Loss: 0.1430\n",
            "Epoch [2/3], Step [5300/10575], Loss: 0.1385\n",
            "Epoch [2/3], Step [5400/10575], Loss: 0.1407\n",
            "Epoch [2/3], Step [5500/10575], Loss: 0.1429\n",
            "Epoch [2/3], Step [5600/10575], Loss: 0.1410\n",
            "Epoch [2/3], Step [5700/10575], Loss: 0.1410\n",
            "Epoch [2/3], Step [5800/10575], Loss: 0.1368\n",
            "Epoch [2/3], Step [5900/10575], Loss: 0.1370\n",
            "Epoch [2/3], Step [6000/10575], Loss: 0.1337\n",
            "Epoch [2/3], Step [6100/10575], Loss: 0.1393\n",
            "Epoch [2/3], Step [6200/10575], Loss: 0.1341\n",
            "Epoch [2/3], Step [6300/10575], Loss: 0.1343\n",
            "Epoch [2/3], Step [6400/10575], Loss: 0.1362\n",
            "Epoch [2/3], Step [6500/10575], Loss: 0.1337\n",
            "Epoch [2/3], Step [6600/10575], Loss: 0.1320\n",
            "Epoch [2/3], Step [6700/10575], Loss: 0.1340\n",
            "Epoch [2/3], Step [6800/10575], Loss: 0.1301\n",
            "Epoch [2/3], Step [6900/10575], Loss: 0.1330\n",
            "Epoch [2/3], Step [7000/10575], Loss: 0.1350\n",
            "Epoch [2/3], Step [7100/10575], Loss: 0.1311\n",
            "Epoch [2/3], Step [7200/10575], Loss: 0.1275\n",
            "Epoch [2/3], Step [7300/10575], Loss: 0.1323\n",
            "Epoch [2/3], Step [7400/10575], Loss: 0.1307\n",
            "Epoch [2/3], Step [7500/10575], Loss: 0.1278\n",
            "Epoch [2/3], Step [7600/10575], Loss: 0.1280\n",
            "Epoch [2/3], Step [7700/10575], Loss: 0.1282\n",
            "Epoch [2/3], Step [7800/10575], Loss: 0.1309\n",
            "Epoch [2/3], Step [7900/10575], Loss: 0.1287\n",
            "Epoch [2/3], Step [8000/10575], Loss: 0.1295\n",
            "Epoch [2/3], Step [8100/10575], Loss: 0.1248\n",
            "Epoch [2/3], Step [8200/10575], Loss: 0.1327\n",
            "Epoch [2/3], Step [8300/10575], Loss: 0.1286\n",
            "Epoch [2/3], Step [8400/10575], Loss: 0.1244\n",
            "Epoch [2/3], Step [8500/10575], Loss: 0.1289\n",
            "Epoch [2/3], Step [8600/10575], Loss: 0.1243\n",
            "Epoch [2/3], Step [8700/10575], Loss: 0.1262\n",
            "Epoch [2/3], Step [8800/10575], Loss: 0.1290\n",
            "Epoch [2/3], Step [8900/10575], Loss: 0.1240\n",
            "Epoch [2/3], Step [9000/10575], Loss: 0.1228\n",
            "Epoch [2/3], Step [9100/10575], Loss: 0.1201\n",
            "Epoch [2/3], Step [9200/10575], Loss: 0.1253\n",
            "Epoch [2/3], Step [9300/10575], Loss: 0.1225\n",
            "Epoch [2/3], Step [9400/10575], Loss: 0.1225\n",
            "Epoch [2/3], Step [9500/10575], Loss: 0.1218\n",
            "Epoch [2/3], Step [9600/10575], Loss: 0.1219\n",
            "Epoch [2/3], Step [9700/10575], Loss: 0.1235\n",
            "Epoch [2/3], Step [9800/10575], Loss: 0.1205\n",
            "Epoch [2/3], Step [9900/10575], Loss: 0.1230\n",
            "Epoch [2/3], Step [10000/10575], Loss: 0.1201\n",
            "Epoch [2/3], Step [10100/10575], Loss: 0.1197\n",
            "Epoch [2/3], Step [10200/10575], Loss: 0.1180\n",
            "Epoch [2/3], Step [10300/10575], Loss: 0.1188\n",
            "Epoch [2/3], Step [10400/10575], Loss: 0.1223\n",
            "Epoch [2/3], Step [10500/10575], Loss: 0.1236\n",
            "üéØ Epoch [2/3] complete ‚Äî Average Loss: 0.1393\n",
            "Epoch [3/3], Step [0/10575], Loss: 0.1230\n",
            "Epoch [3/3], Step [100/10575], Loss: 0.1166\n",
            "Epoch [3/3], Step [200/10575], Loss: 0.1155\n",
            "Epoch [3/3], Step [300/10575], Loss: 0.1140\n",
            "Epoch [3/3], Step [400/10575], Loss: 0.1184\n",
            "Epoch [3/3], Step [500/10575], Loss: 0.1213\n",
            "Epoch [3/3], Step [600/10575], Loss: 0.1108\n",
            "Epoch [3/3], Step [700/10575], Loss: 0.1117\n",
            "Epoch [3/3], Step [800/10575], Loss: 0.1186\n",
            "Epoch [3/3], Step [900/10575], Loss: 0.1179\n",
            "Epoch [3/3], Step [1000/10575], Loss: 0.1122\n",
            "Epoch [3/3], Step [1100/10575], Loss: 0.1148\n",
            "Epoch [3/3], Step [1200/10575], Loss: 0.1138\n",
            "Epoch [3/3], Step [1300/10575], Loss: 0.1095\n",
            "Epoch [3/3], Step [1400/10575], Loss: 0.1139\n",
            "Epoch [3/3], Step [1500/10575], Loss: 0.1149\n",
            "Epoch [3/3], Step [1600/10575], Loss: 0.1125\n",
            "Epoch [3/3], Step [1700/10575], Loss: 0.1154\n",
            "Epoch [3/3], Step [1800/10575], Loss: 0.1126\n",
            "Epoch [3/3], Step [1900/10575], Loss: 0.1080\n",
            "Epoch [3/3], Step [2000/10575], Loss: 0.1143\n",
            "Epoch [3/3], Step [2100/10575], Loss: 0.1124\n",
            "Epoch [3/3], Step [2200/10575], Loss: 0.1075\n",
            "Epoch [3/3], Step [2300/10575], Loss: 0.1140\n",
            "Epoch [3/3], Step [2400/10575], Loss: 0.1080\n",
            "Epoch [3/3], Step [2500/10575], Loss: 0.1132\n",
            "Epoch [3/3], Step [2600/10575], Loss: 0.1114\n",
            "Epoch [3/3], Step [2700/10575], Loss: 0.1113\n",
            "Epoch [3/3], Step [2800/10575], Loss: 0.1082\n",
            "Epoch [3/3], Step [2900/10575], Loss: 0.1070\n",
            "Epoch [3/3], Step [3000/10575], Loss: 0.1076\n",
            "Epoch [3/3], Step [3100/10575], Loss: 0.1109\n",
            "Epoch [3/3], Step [3200/10575], Loss: 0.1125\n",
            "Epoch [3/3], Step [3300/10575], Loss: 0.1083\n",
            "Epoch [3/3], Step [3400/10575], Loss: 0.1101\n",
            "Epoch [3/3], Step [3500/10575], Loss: 0.1115\n",
            "Epoch [3/3], Step [3600/10575], Loss: 0.1064\n",
            "Epoch [3/3], Step [3700/10575], Loss: 0.1087\n",
            "Epoch [3/3], Step [3800/10575], Loss: 0.1031\n",
            "Epoch [3/3], Step [3900/10575], Loss: 0.1056\n",
            "Epoch [3/3], Step [4000/10575], Loss: 0.1064\n",
            "Epoch [3/3], Step [4100/10575], Loss: 0.1020\n",
            "Epoch [3/3], Step [4200/10575], Loss: 0.1086\n",
            "Epoch [3/3], Step [4300/10575], Loss: 0.1085\n",
            "Epoch [3/3], Step [4400/10575], Loss: 0.1033\n",
            "Epoch [3/3], Step [4500/10575], Loss: 0.1054\n",
            "Epoch [3/3], Step [4600/10575], Loss: 0.1069\n",
            "Epoch [3/3], Step [4700/10575], Loss: 0.1061\n",
            "Epoch [3/3], Step [4800/10575], Loss: 0.1063\n",
            "Epoch [3/3], Step [4900/10575], Loss: 0.1002\n",
            "Epoch [3/3], Step [5000/10575], Loss: 0.1021\n",
            "Epoch [3/3], Step [5100/10575], Loss: 0.1070\n",
            "Epoch [3/3], Step [5200/10575], Loss: 0.1025\n",
            "Epoch [3/3], Step [5300/10575], Loss: 0.1061\n",
            "Epoch [3/3], Step [5400/10575], Loss: 0.0976\n",
            "Epoch [3/3], Step [5500/10575], Loss: 0.1015\n",
            "Epoch [3/3], Step [5600/10575], Loss: 0.1034\n",
            "Epoch [3/3], Step [5700/10575], Loss: 0.1017\n",
            "Epoch [3/3], Step [5800/10575], Loss: 0.1022\n",
            "Epoch [3/3], Step [5900/10575], Loss: 0.1008\n",
            "Epoch [3/3], Step [6000/10575], Loss: 0.0997\n",
            "Epoch [3/3], Step [6100/10575], Loss: 0.1023\n",
            "Epoch [3/3], Step [6200/10575], Loss: 0.0995\n",
            "Epoch [3/3], Step [6300/10575], Loss: 0.1011\n",
            "Epoch [3/3], Step [6400/10575], Loss: 0.0986\n",
            "Epoch [3/3], Step [6500/10575], Loss: 0.0975\n",
            "Epoch [3/3], Step [6600/10575], Loss: 0.0974\n",
            "Epoch [3/3], Step [6700/10575], Loss: 0.1014\n",
            "Epoch [3/3], Step [6800/10575], Loss: 0.0946\n",
            "Epoch [3/3], Step [6900/10575], Loss: 0.1027\n",
            "Epoch [3/3], Step [7000/10575], Loss: 0.1008\n",
            "Epoch [3/3], Step [7100/10575], Loss: 0.1002\n",
            "Epoch [3/3], Step [7200/10575], Loss: 0.0985\n",
            "Epoch [3/3], Step [7300/10575], Loss: 0.0978\n",
            "Epoch [3/3], Step [7400/10575], Loss: 0.0999\n",
            "Epoch [3/3], Step [7500/10575], Loss: 0.0994\n",
            "Epoch [3/3], Step [7600/10575], Loss: 0.0953\n",
            "Epoch [3/3], Step [7700/10575], Loss: 0.1017\n",
            "Epoch [3/3], Step [7800/10575], Loss: 0.0990\n",
            "Epoch [3/3], Step [7900/10575], Loss: 0.1015\n",
            "Epoch [3/3], Step [8000/10575], Loss: 0.0965\n",
            "Epoch [3/3], Step [8100/10575], Loss: 0.0984\n",
            "Epoch [3/3], Step [8200/10575], Loss: 0.0997\n",
            "Epoch [3/3], Step [8300/10575], Loss: 0.1011\n",
            "Epoch [3/3], Step [8400/10575], Loss: 0.0979\n",
            "Epoch [3/3], Step [8500/10575], Loss: 0.0988\n",
            "Epoch [3/3], Step [8600/10575], Loss: 0.0942\n",
            "Epoch [3/3], Step [8700/10575], Loss: 0.0935\n",
            "Epoch [3/3], Step [8800/10575], Loss: 0.0973\n",
            "Epoch [3/3], Step [8900/10575], Loss: 0.0975\n",
            "Epoch [3/3], Step [9000/10575], Loss: 0.0994\n",
            "Epoch [3/3], Step [9100/10575], Loss: 0.0959\n",
            "Epoch [3/3], Step [9200/10575], Loss: 0.0964\n",
            "Epoch [3/3], Step [9300/10575], Loss: 0.0963\n",
            "Epoch [3/3], Step [9400/10575], Loss: 0.0901\n",
            "Epoch [3/3], Step [9500/10575], Loss: 0.0941\n",
            "Epoch [3/3], Step [9600/10575], Loss: 0.0963\n",
            "Epoch [3/3], Step [9700/10575], Loss: 0.0960\n",
            "Epoch [3/3], Step [9800/10575], Loss: 0.0962\n",
            "Epoch [3/3], Step [9900/10575], Loss: 0.0976\n",
            "Epoch [3/3], Step [10000/10575], Loss: 0.0930\n",
            "Epoch [3/3], Step [10100/10575], Loss: 0.0946\n",
            "Epoch [3/3], Step [10200/10575], Loss: 0.0961\n",
            "Epoch [3/3], Step [10300/10575], Loss: 0.0969\n",
            "Epoch [3/3], Step [10400/10575], Loss: 0.0960\n",
            "Epoch [3/3], Step [10500/10575], Loss: 0.0922\n",
            "üéØ Epoch [3/3] complete ‚Äî Average Loss: 0.1044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "torch.save(model.state_dict(), \"trained_decoder_model.pth\")\n",
        "print(\"Model training complete and saved as 'trained_decoder_model.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GYCOTVPOWnb",
        "outputId": "32138ed5-87d1-4398-a839-8d1d892ec68b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete and saved as 'trained_decoder_model.pth'\n"
          ]
        }
      ]
    }
  ]
}