# -*- coding: utf-8 -*-
"""train_bpe_tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LBvtgTypSgl-DvKs4Vx1Rw-xQ2MPPEXE
"""

#Install the required library
!pip install tokenizers --quiet

#Import Tokenizer classes
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers
from tokenizers.normalizers import NFKC
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer
import os

#Load and define tokenizer
tokenizer = Tokenizer(models.BPE())
tokenizer.normalizer = normalizers.Sequence([NFKC()])
tokenizer.pre_tokenizer = Whitespace()

#Trainer - we use vocab size 10000 and special tokens
trainer = BpeTrainer(
    vocab_size=10000,
    show_progress=True,
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)

#Train it using your data file
files = ["cleaned_combined_chat_data.txt"]
tokenizer.train(files, trainer)

#Save the tokenizer locally
tokenizer.save("bpe_tokenizer.json")

print(" Tokenizer trained and saved as 'bpe_tokenizer.json'")