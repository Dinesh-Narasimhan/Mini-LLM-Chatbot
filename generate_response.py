# -*- coding: utf-8 -*-
"""generate_response.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/198F_QlvjfDi2HqiCGOhrmoNVUA2wR-7q
"""

import json
import torch
import torch.nn.functional as F
from transformer_decoder_model import TransformerDecoderModel, Config

# --------- Load tokenizer ---------
with open("bpe_tokenizer.json", "r", encoding="utf-8") as f:
    tokenizer_data = json.load(f)

vocab = tokenizer_data["model"]["vocab"]
merges = tokenizer_data.get("model", {}).get("merges", [])
inv_vocab = {v: k for k, v in vocab.items()}

# --------- Clean & prepare vocab ---------
def basic_pre_tokenize(text):
    return text.strip().split()

def encode(text):
    tokens = []
    words = basic_pre_tokenize(text)
    for word in words:
        if word in vocab:
            tokens.append(vocab[word])
        else:
            subword = []
            i = 0
            while i < len(word):
                for j in range(len(word), i, -1):
                    sub = word[i:j]
                    if sub in vocab:
                        subword.append(vocab[sub])
                        i = j
                        break
                else:
                    subword.append(vocab.get('[UNK]', 1))
                    break
            tokens.extend(subword)
    return tokens

def decode(token_ids):
    return ' '.join([inv_vocab.get(i, '[UNK]') for i in token_ids if i in inv_vocab])

# --------- Load model ---------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
config = Config()
model = TransformerDecoderModel(config).to(device)
model.load_state_dict(torch.load("trained_decoder_model.pth", map_location=device))
model.eval()

# --------- Generation function ---------
@torch.no_grad()
def generate_response(prompt, max_new_tokens=50):
    input_ids = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)

    for _ in range(max_new_tokens):
        idx_cond = input_ids[:, -config.block_size:]
        logits = model(idx_cond)
        logits = logits[:, -1, :]
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        input_ids = torch.cat([input_ids, next_token], dim=1)

        # If [SEP] or newline-style end token, stop generation
        if next_token.item() in [
            vocab.get('[SEP]', -1),
            vocab.get('[PAD]', -1),
            vocab.get('User', -1)
        ]:
            break

    # Get only Bot's part of the output
    output = decode(input_ids[0].tolist())
    if "Bot:" in output:
        output = output.split("Bot:")[-1]
    return output.strip().replace('[PAD]', '').replace('[SEP]', '').strip()


# --------- Chat loop ---------
if __name__ == "__main__":
    print("ðŸ’¬ Chatbot ready. Type 'exit' to quit.")
    while True:
        user_input = input("User: ").strip()
        if user_input.lower() in ["exit", "quit"]:
            break
        prompt = f"User: {user_input} Bot:"
        response = generate_response(prompt)
        print("Bot:", response)